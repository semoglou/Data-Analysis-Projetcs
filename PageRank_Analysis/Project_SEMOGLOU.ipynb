{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3699506",
   "metadata": {},
   "source": [
    "## Business Mathematics - Data Analysis with Python, Project 3\n",
    "\n",
    "## PageRank Web Graph Analysis \n",
    "\n",
    "### Angelos Semoglou, s3332318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7665c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy import sparse\n",
    "import time\n",
    "\n",
    "# Suppressing RuntimeWarning to avoid unnecessary warnings during execution\n",
    "# ZeroDivisionError is expected during the computation and is handled appropriately\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d3abb",
   "metadata": {},
   "source": [
    "### Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebddf244",
   "metadata": {},
   "source": [
    "#### Data Retrieval and Modification Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ca7db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data):\n",
    "    \"\"\"\n",
    "    Extracts and processes data from the given file.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the data file/ Name of the file (str).\n",
    "\n",
    "    Returns:\n",
    "    - edges: The number of edges in the \"graph\".\n",
    "    - nodes: The number of nodes in the \"graph\".\n",
    "    - incoming_edges: Dictionary with webpages as keys and the count of their outgoing edges as values.\n",
    "    - outgoing_edges: Dictionary with webpages as keys and a list of incoming edges as values.\n",
    "    - source_pages: List of the origin pages.\n",
    "    - target_pages: List of the destination pages.\n",
    "    \"\"\"\n",
    "    # source_pages: list to store the source/origin pages.\n",
    "    # target_pages: list to store the target/destination pages.\n",
    "    source_pages, target_pages = [], []\n",
    "    \n",
    "    # incoming_edges: Dictionary with webpages as keys and the count of their outgoing edges as values.\n",
    "    # outgoing_edges: Dictionary with webpages as keys and a list of incoming edges as values.\n",
    "    incoming_edges, outgoing_edges = {}, {}\n",
    "    \n",
    "    # Open the data file for reading.\n",
    "    with open(data, 'r') as f:\n",
    "        # Iterate over each line in the initial data file.\n",
    "        for line in f.readlines():\n",
    "            # Extract the item from the first column and subtract one.\n",
    "            # This adjustment is made because webpage IDs start from 1,\n",
    "            # but matrix indexes start from 0 (Same for the 2nd column).\n",
    "            source = int(line.split()[0]) - 1\n",
    "            destination = int(line.split()[1]) - 1\n",
    "            \n",
    "            source_pages.append(source)\n",
    "            target_pages.append(destination)\n",
    "            \n",
    "            # Fill the outgoing_edges dictionary by counting how many times\n",
    "            # each source page has outgoing edges.\n",
    "            if source not in outgoing_edges.keys():\n",
    "                outgoing_edges[source] = 1\n",
    "            else:\n",
    "                outgoing_edges[source] += 1\n",
    "                \n",
    "            # Fill the incoming_edges dictionary by tracking the source pages\n",
    "            # for each target/destination page\n",
    "            if destination not in incoming_edges.keys():\n",
    "                incoming_edges[destination] = [source]\n",
    "            else:\n",
    "                incoming_edges[destination].append(source) \n",
    "                \n",
    "    # Compute number of edges\n",
    "    edges = len(source_pages)\n",
    "    \n",
    "    # Compute the number of nodes by taking the union of unique source and target pages\n",
    "    nodes = len(set(source_pages) | set(target_pages))\n",
    "    \n",
    "    # Ensure that all nodes have entries in the incoming_edges dictionary\n",
    "    for node in range(nodes):  \n",
    "        if node not in incoming_edges.keys():\n",
    "            incoming_edges[node] = []\n",
    "\n",
    "    # Return the computed values\n",
    "    return edges, nodes, incoming_edges, outgoing_edges, source_pages, target_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18232f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_add_X(data):\n",
    "    \"\"\"\n",
    "    Load data from a file and add a node X without inlinks or outlinks.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the data file/ Data file name.\n",
    "\n",
    "    Returns:\n",
    "    - edges: The updated number of edges.\n",
    "    - nodes: The updated number of nodes.\n",
    "    - source_pages: List of origin pages.\n",
    "    - target_pages: List of destination pages.\n",
    "    \"\"\"\n",
    "    source_pages, target_pages = [], []\n",
    "    \n",
    "    with open(data, 'r') as f:\n",
    "        # Extract source and destination pages from each line.\n",
    "        for line in f.readlines():\n",
    "            source = int(line.split()[0]) - 1\n",
    "            destination = int(line.split()[1]) - 1\n",
    "            source_pages.append(source)\n",
    "            target_pages.append(destination)\n",
    "            \n",
    "    edges = len(source_pages)\n",
    "    nodes = len(set(source_pages) | set(target_pages))\n",
    "    # Increment the number of nodes to account for the new node X.\n",
    "    nodes += 1\n",
    "    \n",
    "    return edges, nodes, source_pages, target_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ccd164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_add_XY(data):\n",
    "    \"\"\"\n",
    "    Load data from a file and add two new nodes X, Y and a new edge Y -> X.\n",
    "    (Y links to X)\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the data file/ Data file name.\n",
    "\n",
    "    Returns:\n",
    "    - edges: Number of edges in the graph.\n",
    "    - nodes: Number of nodes in the graph.\n",
    "    - source_pages: List to store the origin pages.\n",
    "    - target_pages: List to store the destination pages.\n",
    "    \"\"\"\n",
    "    source_pages, target_pages = [], []\n",
    "\n",
    "    with open(data, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            source = int(line.split()[0]) - 1\n",
    "            destination = int(line.split()[1]) - 1\n",
    "            source_pages.append(source)\n",
    "            target_pages.append(destination)\n",
    "            \n",
    "    edges = len(source_pages)\n",
    "    nodes = len(set(source_pages) | set(target_pages))\n",
    "    \n",
    "    # Add the new edge Y -> X\n",
    "    source_pages.append(nodes)\n",
    "    target_pages.append(nodes + 1)\n",
    "    \n",
    "    # Increase the number of edges by one\n",
    "    edges += 1\n",
    "    # Increase the number of nodes by two for X, Y\n",
    "    nodes += 2\n",
    "    \n",
    "    return edges, nodes, source_pages, target_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d1bfd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_add_XYZ(data):\n",
    "    \"\"\"\n",
    "    Load data from a file and add three nodes X, Y, Z and two new edges (Y -> X and Z -> X)\n",
    "    to the graph, updating the number of nodes and edges accordingly.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the data file/ Data file name.\n",
    "\n",
    "    Returns:\n",
    "    - edges: The updated number of edges.\n",
    "    - nodes: The updated number of nodes.\n",
    "    - source_pages: List of origin pages.\n",
    "    - target_pages: List of destination pages.\n",
    "    \"\"\"\n",
    "    source_pages, target_pages = [], []\n",
    "\n",
    "    with open(data, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            source = int(line.split()[0]) - 1\n",
    "            destination = int(line.split()[1]) - 1\n",
    "            source_pages.append(source)\n",
    "            target_pages.append(destination)\n",
    "\n",
    "    edges = len(source_pages)\n",
    "    nodes = len(set(source_pages) | set(target_pages))\n",
    "\n",
    "    # New edge Y -> X\n",
    "    source_pages.append(nodes)\n",
    "    target_pages.append(nodes + 2)\n",
    "\n",
    "    # New edge Z -> X\n",
    "    source_pages.append(nodes + 1)\n",
    "    target_pages.append(nodes + 2)\n",
    "\n",
    "    nodes += 3\n",
    "    edges += 2\n",
    "\n",
    "    return edges, nodes, source_pages, target_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6482ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_add_XYZ_popular_1(data, popular):\n",
    "    \"\"\"\n",
    "    Load data from a file, add nodes X, Y, Z, and links from X to popular pages.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the data file/ Data file name.\n",
    "    - popular: List of popular nodes to link with node X.\n",
    "\n",
    "    Returns:\n",
    "    - edges: The updated number of edges.\n",
    "    - nodes: The updated number of nodes.\n",
    "    - source_pages: List of origin pages.\n",
    "    - target_pages: List of destination pages.\n",
    "    \"\"\"\n",
    "    source_pages, target_pages = [], []\n",
    "    with open(data, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            source = int(line.split()[0]) - 1\n",
    "            destination = int(line.split()[1]) - 1\n",
    "            source_pages.append(source)\n",
    "            target_pages.append(destination)\n",
    "            \n",
    "    edges = len(source_pages)\n",
    "    nodes = len(set(source_pages) | set(target_pages))\n",
    "    \n",
    "    source_pages.append(nodes)\n",
    "    target_pages.append(nodes + 2)\n",
    "    \n",
    "    source_pages.append(nodes + 1)\n",
    "    target_pages.append(nodes + 2)\n",
    "    \n",
    "    # Add links from X to older, popular pages\n",
    "    for node in popular:\n",
    "        source_pages.append(nodes + 2)\n",
    "        target_pages.append(node)\n",
    "    \n",
    "    # Increase number of nodes and edges as needed\n",
    "    nodes += 3\n",
    "    edges += len(popular) + 2\n",
    "    \n",
    "    return edges, nodes, source_pages, target_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9597362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_add_XYZ_popular_2(data, popular):\n",
    "    \"\"\"\n",
    "    Load data from a file, add nodes X, Y, Z, and links from Y or Z to popular pages.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the data file/ Data file name.\n",
    "    - popular: List of popular nodes.\n",
    "\n",
    "    Returns:\n",
    "    - edges: The updated number of edges.\n",
    "    - nodes: The updated number of nodes.\n",
    "    - source_pages: List of origin pages.\n",
    "    - target_pages: List of destination pages.\n",
    "    \"\"\"\n",
    "    source_pages, target_pages = [], []\n",
    "    with open(data, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            source = int(line.split()[0]) - 1\n",
    "            destination = int(line.split()[1]) - 1\n",
    "            source_pages.append(source)\n",
    "            target_pages.append(destination)\n",
    "            \n",
    "    edges = len(source_pages)\n",
    "    nodes = len(set(source_pages) | set(target_pages))\n",
    "    \n",
    "    source_pages.append(nodes)\n",
    "    target_pages.append(nodes + 2)\n",
    "    \n",
    "    source_pages.append(nodes + 1)\n",
    "    target_pages.append(nodes + 2)\n",
    "    \n",
    "    # add links from X to older, popular pages\n",
    "    for node in popular:\n",
    "        source_pages.append(nodes + 2) # Assuming it's node Z, change to (nodes + 1) for Y\n",
    "        target_pages.append(node)\n",
    "    \n",
    "    # increase number of nodes and edges as needed\n",
    "    nodes += 3\n",
    "    edges += len(popular) + 2\n",
    "\n",
    "    return edges, nodes, source_pages, target_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63523f",
   "metadata": {},
   "source": [
    "#### Sparse Matrix Construction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8713788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sparse_matrix(data):\n",
    "    \"\"\"\n",
    "    Construct a sparse matrix from the given connectivity data.\n",
    "    (sparse matrix is used for memory efficiency)\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The path to the input data file/ Data file name.\n",
    "\n",
    "    Returns:\n",
    "    - sparse_matrix (sparse.csr_matrix): The constructed Sparse-CSR matrix - representation of the graph.\n",
    "    \"\"\"\n",
    "    edges, nodes, _, _, source_pages, target_pages = get_data(data)\n",
    "    \n",
    "    edge_weights = [1]*edges\n",
    "    sparse_matrix = sparse.csr_matrix((edge_weights,\n",
    "                                   (target_pages, source_pages)),\n",
    "                                   shape=(nodes, nodes))\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd717dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sparse_matrix_X(data):\n",
    "    \"\"\"\n",
    "    Construct a sparse matrix for the modified graph\n",
    "    with an additional node X without inlinks or outlinks.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the input data file.\n",
    "\n",
    "    Returns:\n",
    "    - The sparse matrix representation of the graph.\n",
    "    \"\"\"\n",
    "    # Get data for the modified graph with an additional node X\n",
    "    edges, nodes, source_pages, target_pages = get_data_add_X(data)\n",
    "    \n",
    "    edge_weights = [1]*edges\n",
    "    sparse_matrix = sparse.csr_matrix((edge_weights,\n",
    "                                   (target_pages, source_pages)),\n",
    "                                   shape=(nodes, nodes))\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a58aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sparse_matrix_XY(data):\n",
    "    \"\"\"\n",
    "    Construct a sparse matrix for the modified graph\n",
    "    with two additional nodes X, Y and an edge from Y to X.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the input data file.\n",
    "\n",
    "    Returns:\n",
    "    - The sparse matrix representation of the graph.\n",
    "    \"\"\"\n",
    "    # Get data for the modified graph with an additional edge from Y to X\n",
    "    edges, nodes, source_pages, target_pages = get_data_add_XY(data)\n",
    "    \n",
    "    edge_weights = [1]*edges\n",
    "    sparse_matrix = sparse.csr_matrix((edge_weights,\n",
    "                                   (target_pages, source_pages)),\n",
    "                                   shape=(nodes, nodes))\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b30c0e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sparse_matrix_XYZ(data):\n",
    "    \"\"\"\n",
    "    Construct a sparse matrix for the modified graph \n",
    "    with three additional nodes X, Y ,Z and edges from Y and Z to X.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the input data file.\n",
    "\n",
    "    Returns:\n",
    "    -The sparse matrix representation of the graph.\n",
    "    \"\"\"\n",
    "    # Get data for the modified graph with additional edges from Y and Z to X\n",
    "    edges, nodes, source_pages, target_pages = get_data_add_XYZ(data)\n",
    "\n",
    "    edge_weights = [1]*edges\n",
    "    sparse_matrix = sparse.csr_matrix((edge_weights,\n",
    "                                   (target_pages, source_pages)),\n",
    "                                   shape=(nodes, nodes))\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee5f5dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sparse_matrix_popular_1(data, popular):\n",
    "    \"\"\"\n",
    "    Construct a sparse matrix based on the provided data, emphasizing popular nodes.\n",
    "    (nodes X, Y, Z, and links from X to popular pages)\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The input data file.\n",
    "    - popular: A list of popular nodes.\n",
    "\n",
    "    Returns:\n",
    "    - A sparse matrix representing the graph.\n",
    "    \"\"\"\n",
    "    edges, nodes, source_pages, target_pages = get_data_add_XYZ_popular_1(data, popular)\n",
    "    edge_weights = [1]*edges\n",
    "    sparse_matrix = sparse.csr_matrix((edge_weights, (target_pages, source_pages)), shape=(nodes, nodes))\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "685e892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sparse_matrix_popular_2(data, popular):\n",
    "    \"\"\"\n",
    "    Construct a sparse matrix based on the provided data, emphasizing popular nodes.\n",
    "    (nodes X, Y, Z, and links from Z to popular pages)\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The input data file.\n",
    "    - popular: A list of popular nodes.\n",
    "\n",
    "    Returns:\n",
    "    - A sparse matrix representing the graph.\n",
    "    \"\"\"\n",
    "    edges, nodes, source_pages, target_pages = get_data_add_XYZ_popular_2(data, popular)\n",
    "    \n",
    "    edge_weights = [1]*edges\n",
    "    sparse_matrix = sparse.csr_matrix((edge_weights,\n",
    "                                   (target_pages, source_pages)),\n",
    "                                   shape=(nodes, nodes))\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1de69",
   "metadata": {},
   "source": [
    "#### PageRank Algorithm Implementation with Power Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64e3d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PageRank_PowerMethod(G, alpha, tolerance = 1e-8):\n",
    "    \"\"\"\n",
    "    Compute PageRank using the Power Method.\n",
    "\n",
    "    Parameters:\n",
    "    - G: The adjacency matrix of the graph.\n",
    "    - alpha: The teleport probability.\n",
    "    - tolerance: The convergence tolerance. Defaults to 1e-8.\n",
    "\n",
    "    Returns:\n",
    "    - ranks: The computed PageRank vector.\n",
    "    - iterations: The number of iterations performed.\n",
    "    - converged_nodes: List of nodes that converged in the first iteration.\n",
    "    \"\"\"\n",
    "    n = G.shape[0]\n",
    "\n",
    "    # Number of outlinks in each node-webpage\n",
    "    d = G.sum(axis=0).T \n",
    "    \n",
    "    error = np.inf   # Initialize error to infinity\n",
    "    iterations = 0   # Iteration counter\n",
    "    converged_nodes = []   # List to store nodes that converged in the first iteration\n",
    "    \n",
    "    ranks = np.ones((n,1))/n   # Initialize the vector for the algorithm\n",
    "    \n",
    "    \n",
    "    while error > tolerance:       \n",
    "        try:\n",
    "            new_ranks = G.dot(alpha * (ranks / d))  # G.dot(1/d) is the stochastic matrix\n",
    "        except (ZeroDivisionError): \n",
    "            pass  \n",
    "        new_ranks += (1 - alpha) / n\n",
    "        \n",
    "        # Normalize using L1 Norm (Manhattan Distance) to ensure that the ranks sum to 1.\n",
    "        new_ranks = new_ranks / np.linalg.norm(new_ranks, ord=1) \n",
    "\n",
    "        iterations += 1\n",
    "        \n",
    "        # Identify the nodes that converge in the first iteration\n",
    "        if iterations < 2:\n",
    "            for node in range(len(ranks)):\n",
    "                if np.linalg.norm(ranks[node]-new_ranks[node]) < tolerance:\n",
    "                    converged_nodes.append([node])\n",
    "                    \n",
    "        # Check the stopping condition\n",
    "        error = np.linalg.norm(ranks - new_ranks)/np.linalg.norm(ranks)  \n",
    "        ranks = new_ranks\n",
    "\n",
    "    return new_ranks, iterations, converged_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdc6640",
   "metadata": {},
   "source": [
    "### Question 1. PageRank Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4612429",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = construct_sparse_matrix('stanweb.dat')\n",
    "n = G.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d556b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank with alpha = 0.85\n",
      "Duration: 4.60 seconds\n",
      "Number of iterations: 95\n",
      "\n",
      "PageRank vector:\n",
      "\n",
      " [[5.32288161e-07]\n",
      " [1.17232693e-04]\n",
      " [8.25934227e-07]\n",
      " ...\n",
      " [5.35938215e-07]\n",
      " [1.80768199e-06]\n",
      " [1.47755921e-06]]\n",
      "\n",
      " Top 100 ranked nodes are:\n",
      "\n",
      " [ 89072 226410 241453 262859 134831 234703 136820  68888 105606  69357\n",
      "  67755 225871 186749 272441 251795  95162 119478 231362  55787 167294\n",
      " 179644  38341 117151 198089  60209 235495 132694 181700 247240 259454\n",
      "  62477 120707 161889  77998  17780 176789 183003 221086 137631  96744\n",
      " 112741 145891 151427  60439  81434 208541     90 258347 214127 222872\n",
      "  27903 272761  96357  93777  34572 158567 192119 227978 245658 118243\n",
      "  28599 104766  18545  13238 101160  65462  32103 279367  65579 185471\n",
      " 170451  38948  93988 273988  84427 186901   2259  52021 205476 134374\n",
      "  49101 151980  17566 173904  19188 137797 174664  84905 113672 208085\n",
      "  67502 210161 153025 272960  47257  36368  89776 184124  59589  66693]\n"
     ]
    }
   ],
   "source": [
    "start =  time.time()\n",
    "\n",
    "# Compute PageRank using Power Method\n",
    "ranks, iterations, fast = PageRank_PowerMethod(G, 0.85, 1e-8)\n",
    "\n",
    "end = time.time()\n",
    "duration = format(end - start, '.2f')\n",
    "\n",
    "print('PageRank with alpha = 0.85')\n",
    "print(f'Duration: {duration} seconds')\n",
    "print(f'Number of iterations: {iterations}')\n",
    "print(f'\\nPageRank vector:\\n\\n {ranks}')\n",
    "\n",
    "ranks = np.asarray(ranks).ravel()\n",
    "nodes_alpha_85 = ranks.argsort()[-n:][::-1]\n",
    "print(f'\\n Top 100 ranked nodes are:\\n\\n {nodes_alpha_85[:100]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0501a",
   "metadata": {},
   "source": [
    "### Question 2. PageRank Vectors with Different Alpha Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cab2a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PageRank with alpha = 0.75\n",
      "Duration: 3.60 seconds\n",
      "Number of iterations: 95\n",
      "\n",
      "PageRank vector:\n",
      "\n",
      " [[8.87133439e-07]\n",
      " [1.02190903e-04]\n",
      " [1.20202886e-06]\n",
      " ...\n",
      " [8.92501002e-07]\n",
      " [2.07445288e-06]\n",
      " [1.93160895e-06]]\n",
      "\n",
      "Top 100 ranked nodes are:\n",
      "\n",
      " [226410  89072 241453 134831  67755  69357 225871 234703 186749 231362\n",
      " 105606 136820  68888 167294 262859  38341 119478  95162 251795 272441\n",
      "  55787 198089  81434 214127  93777  34572 245658  60209 117151 158567\n",
      " 258347 132694 235495 101160 179644 181700 259454 247240 120707  62477\n",
      " 137631 221086 176789 183003  77998  17780  96744  27903 272761 112741\n",
      " 161889 170451 151427 145891  65462  60439 208541     90 222872  96357\n",
      " 134374 227978  28599 104766  18545  13238  93988 118243 192119 151980\n",
      " 279367  32103  67502 186901   2259 205476  84427 185471  19188 278082\n",
      "  38948 273988  52021 137797 208085 133796  84905  98386 173904 177306\n",
      " 113672  49101  17566  50784 267491  68294 166975  96386 203321   3407]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "PageRank with alpha = 0.8\n",
      "Duration: 3.78 seconds\n",
      "Number of iterations: 95\n",
      "\n",
      "PageRank vector:\n",
      "\n",
      " [[7.09713890e-07]\n",
      " [1.09075249e-04]\n",
      " [1.01998003e-06]\n",
      " ...\n",
      " [7.14294302e-07]\n",
      " [1.95675022e-06]\n",
      " [1.72541718e-06]]\n",
      "\n",
      "Top 100 ranked nodes are:\n",
      "\n",
      " [ 89072 226410 241453 134831  69357  67755 234703 225871 186749 262859\n",
      " 105606 136820  68888 231362 272441 251795  95162 167294 119478  38341\n",
      "  55787 198089 179644  81434 214127 117151  93777  60209  34572 132694\n",
      " 235495 245658 258347 158567 181700 259454 247240 120707  62477 176789\n",
      " 183003 137631 221086  77998  17780  96744  27903 272761 101160 112741\n",
      " 161889 145891 151427  60439 208541     90 222872  96357 227978  65462\n",
      "  28599 104766 192119  18545  13238 170451 118243 279367  32103 134374\n",
      "  93988 185471 151980 186901  84427   2259 205476  38948 273988  67502\n",
      "  19188  52021 137797  49101  17566 173904  65579 278082 208085  84905\n",
      " 113672 133796  98386 177306 210161 153025 272960  47257   3407 203321]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "PageRank with alpha = 0.9\n",
      "Duration: 5.53 seconds\n",
      "Number of iterations: 95\n",
      "\n",
      "PageRank vector:\n",
      "\n",
      " [[3.54856905e-07]\n",
      " [1.27134963e-04]\n",
      " [6.14462944e-07]\n",
      " ...\n",
      " [3.57433386e-07]\n",
      " [1.59036412e-06]\n",
      " [1.16279102e-06]]\n",
      "\n",
      "Top 100 ranked nodes are:\n",
      "\n",
      " [ 89072 226410 241453 262859 136820  68888 134831 179644 234703  95162\n",
      " 272441 251795 105606 119478  55787 225871 186749  69357  67755 167294\n",
      " 231362 117151  38341 235495 161889 181700 112741 145891 151427  60209\n",
      " 132694 259454 247240  60439  62477 120707 183003 137631 176789  17780\n",
      "  77998 221086  96744     90 208541  65579 222872  96357 192119  27903\n",
      " 272761 198089 118243 227978  32103 258347 174664  28599 104766  18545\n",
      "  13238 279367 281771  81434  65462 214127 185471 158567  93777  38948\n",
      "  34572  49101  17566 273988 173904  52021  77083 245658 186901 101160\n",
      "  84427   2259 205476  93988  36368  19188 170451 210161 113672 123954\n",
      " 184124  66693 272960  47257 153025  74360  89776 233601 247955  84905]\n",
      "\n",
      "-----------------------------------------------------------------------\n",
      "\n",
      "PageRank with alpha = 0.95\n",
      "Duration: 7.57 seconds\n",
      "Number of iterations: 95\n",
      "\n",
      "PageRank vector:\n",
      "\n",
      " [[1.77422132e-07]\n",
      " [1.36754855e-04]\n",
      " [3.75034721e-07]\n",
      " ...\n",
      " [1.78781845e-07]\n",
      " [1.19103284e-06]\n",
      " [7.24616211e-07]]\n",
      "\n",
      "Top 100 ranked nodes are:\n",
      "\n",
      " [ 89072 226410 241453 262859 179644 136820  68888 272441 251795  95162\n",
      " 281771 174664 119478 234703  65579 105606  55787 134831 117151 235495\n",
      "  77083 181700 247240 259454  62477 120707  77998  17780 183003 176789\n",
      " 137631 221086  96744 161889 145891 225871 112741 151427  60439     90\n",
      " 192119 186749 208541 222872  96357  60209  27903 272761 167294 132694\n",
      "  32103 118243  69357  38341  67755 271408  14784 231362 227978 229579\n",
      "  95365  28599 104766  18545  13238 173904 185471  49101  17566 152336\n",
      "  36368 279367  38948  65462 123954 273988  52021  74360 198089 210161\n",
      " 258347  66693 184124 272960  47257 186901  89776   2259 233601 247955\n",
      " 205476 113672 153025  84427  19188  14355  59589 205331 161513 167817]\n",
      "\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# List of alpha values to test\n",
    "alpha_values = [0.75, 0.80, 0.90, 0.95]\n",
    "\n",
    "# Lists to store indices/nodes of the ranks vector for each alpha\n",
    "ind_nodes_list = []\n",
    "\n",
    "# Iterate over each alpha value\n",
    "for alpha in alpha_values:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Compute PageRank using Power Method\n",
    "    ranks = PageRank_PowerMethod(G, alpha, 1e-8)[0]\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = format(end_time - start_time, '.2f')\n",
    "    \n",
    "    # Display results for the current alpha\n",
    "    print(f'\\nPageRank with alpha = {alpha}')\n",
    "    print(f'Duration: {duration} seconds')\n",
    "    print(f'Number of iterations: {iterations}')\n",
    "    print(f'\\nPageRank vector:\\n\\n {ranks}')\n",
    "    \n",
    "    # Get the indices of the ranked nodes\n",
    "    ranks = np.asarray(ranks).ravel()\n",
    "    \n",
    "    alpha_nodes = ranks.argsort()[-n:][::-1]\n",
    "    print('\\nTop 100 ranked nodes are:\\n\\n', alpha_nodes[:100])\n",
    "    \n",
    "    # Save the indices - nodes for later comparison\n",
    "    ind_nodes_list.append(alpha_nodes)\n",
    "    \n",
    "    print('\\n' + '-'*71)  # Separator for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af6c1cb",
   "metadata": {},
   "source": [
    "We observe that certain rankings remain consistent, while others show variations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44674e",
   "metadata": {},
   "source": [
    "A comparison between the results obtained with alpha = 0.85 and those of other alpha values (0.75, 0.80, 0.90, 0.95) will be performed to identify any changes in the rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "860617a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison with alpha = 0.75 and alpha = 0.85:\n",
      "The number of different nodes in the top 100 rankings is: 98\n",
      "\n",
      "Comparison with alpha = 0.8 and alpha = 0.85:\n",
      "The number of different nodes in the top 100 rankings is: 93\n",
      "\n",
      "Comparison with alpha = 0.9 and alpha = 0.85:\n",
      "The number of different nodes in the top 100 rankings is: 91\n",
      "\n",
      "Comparison with alpha = 0.95 and alpha = 0.85:\n",
      "The number of different nodes in the top 100 rankings is: 95\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each alpha value and compare its top 100 nodes with alpha = 0.85\n",
    "for i, nodes  in enumerate(ind_nodes_list):\n",
    "    print(f'\\nComparison with alpha = {alpha_values[i]} and alpha = 0.85:')\n",
    "    \n",
    "    # Compare two lists element-wise and print the number of differing elements\n",
    "    counter = 0\n",
    "    for j in range(100):\n",
    "        if nodes_alpha_85[j] != nodes[j]:\n",
    "            counter += 1\n",
    "    \n",
    "    if counter == 0:\n",
    "        print('The top 100 ranked nodes are the same for all alpha values.')\n",
    "    else:\n",
    "        print('The number of different nodes in the top 100 rankings is:', counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a8f16d",
   "metadata": {},
   "source": [
    "### Question 3. Speed of Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e285bb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes that converged after the first iteration: 14534\n",
      "Number of highly ranked nodes (top 1000) that converged: 5\n",
      "Highly ranked nodes (top 1000) that converged after the first iteration: [35155, 1692, 169244, 188854, 116529]\n"
     ]
    }
   ],
   "source": [
    "ranks, iterations, converged_nodes = PageRank_PowerMethod(G, 0.85, 1e-8)\n",
    "\n",
    "# Get indices of the top ranked nodes\n",
    "ranks = np.asarray(ranks).ravel()\n",
    "top_ranked_indices = ranks.argsort()[-G.shape[0]:][::-1]\n",
    "\n",
    "print('Number of nodes that converged after the first iteration:', len(converged_nodes))\n",
    "\n",
    "# Check if any of the top 1000 highly ranked nodes converged in the first iteration\n",
    "flag = False\n",
    "converged_in_top_1000 = [index for index in top_ranked_indices[:1000] if index in converged_nodes]\n",
    "\n",
    "if converged_in_top_1000:\n",
    "    print(f'Number of highly ranked nodes (top 1000) that converged: {len(converged_in_top_1000)}')\n",
    "    print('Highly ranked nodes (top 1000) that converged after the first iteration:', converged_in_top_1000)\n",
    "else:\n",
    "    print('\\nNone of the top 100 highly ranked nodes converged after the first iteration.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19f666f",
   "metadata": {},
   "source": [
    "### Question 4. PageRank Analysis with Addition of New Web Page X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcafe392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PageRank of page X is: 5.32286513760914e-07\n",
      "Change in PageRank of the other pages due to the addition of the new page: 1.8883261083181873e-08\n",
      "Changes in: 66214 ranks.\n"
     ]
    }
   ],
   "source": [
    "# Construct the adjacency matrix for the original graph\n",
    "G = construct_sparse_matrix('stanweb.dat')\n",
    "\n",
    "# Construct the adjacency matrix for the graph with an additional node X (no in-links or out-links)\n",
    "G_X = construct_sparse_matrix_X('stanweb.dat')\n",
    "\n",
    "# Compute PageRank for the original graph\n",
    "ranks = PageRank_PowerMethod(G, 0.85, 1e-8)[0]\n",
    "# Compute PageRank for the graph with an additional node X\n",
    "ranks_X = PageRank_PowerMethod(G_X, 0.85, 1e-8)[0]\n",
    "\n",
    "# Extract the PageRank of the added node X\n",
    "pagerank_X = np.asarray(ranks_X[G_X.shape[0] - 1]).ravel()[0]\n",
    "print('The PageRank of page X is:', pagerank_X)\n",
    "\n",
    "# Trim ranks_X to match the length of ranks for comparison\n",
    "ranks_X = ranks_X[:len(ranks)]\n",
    "ranks = np.asarray(ranks).ravel()\n",
    "ranks_X = np.asarray(ranks_X).ravel()\n",
    "\n",
    "# Calculate the change in PageRank of the other pages due to the addition of the new page X\n",
    "difference = np.linalg.norm(ranks - ranks_X)\n",
    "print('Change in PageRank of the other pages due to the addition of the new page:', difference)\n",
    "\n",
    "# Get the indices of the top-ranked nodes for both the original and modified graphs\n",
    "indices_ranks = ranks.argsort()[-len(ranks):][::-1]\n",
    "indices_ranks_X = ranks_X.argsort()[-len(ranks_X):][::-1]\n",
    "\n",
    "# Changes in ranks\n",
    "print('Changes in:', np.sum(indices_ranks[:] != indices_ranks_X[:]), 'ranks.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e868386c",
   "metadata": {},
   "source": [
    "We notice that the introduction of the new page has minimal impact on the PageRanks of the existing pages since they have no connections to the newly added page. It is crucial to consider that the total number of nodes is significantly large. The PageRank of the newly added page is influenced by the calculation formula for PageRank.\n",
    "Essentially, this contribution is solely derived from the term (1 - alpha)/n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33228452",
   "metadata": {},
   "source": [
    "### Question 5. PageRank Impact Analysis with Addition of New Web Page Y and Link to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8723acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank of page X after the addition of the new page Y is: 9.848890761537594e-07\n",
      "The PageRank of page Y is: 5.322848301781068e-07\n",
      "Change in PageRank of the other pages due to the addition of the new pages: 4.2346278627566204e-08\n",
      "Changes in: 69781 ranks.\n"
     ]
    }
   ],
   "source": [
    "G = construct_sparse_matrix('stanweb.dat')\n",
    "G_XY = construct_sparse_matrix_XY('stanweb.dat')\n",
    "\n",
    "ranks = PageRank_PowerMethod(G, 0.85, 1e-8)[0]\n",
    "ranks_XY = PageRank_PowerMethod(G_XY, 0.85, 1e-8)[0]\n",
    "\n",
    "# Extract the PageRanks of pages X and Y after the addition of Y\n",
    "\n",
    "pagerank_X = np.asarray(ranks_XY[G_XY.shape[0] - 1]).ravel()[0]\n",
    "print('PageRank of page X after the addition of the new page Y is:', pagerank_X)\n",
    "\n",
    "pagerank_Y = np.asarray(ranks_XY[G_XY.shape[0] - 2]).ravel()[0]\n",
    "print('The PageRank of page Y is:', pagerank_Y)\n",
    "\n",
    "# Trim ranks_XY for comparison\n",
    "ranks_XY = ranks_XY[:len(ranks)]\n",
    "ranks = np.asarray(ranks).ravel()\n",
    "ranks_XY = np.asarray(ranks_XY).ravel()\n",
    "\n",
    "difference = np.linalg.norm(ranks - ranks_XY)\n",
    "print('Change in PageRank of the other pages due to the addition of the new pages:', difference)\n",
    "\n",
    "indices_ranks = ranks.argsort()[-len(ranks):][::-1]\n",
    "indices_ranks_XY = ranks_XY.argsort()[-len(ranks_XY):][::-1]\n",
    "\n",
    "print('Changes in:', np.sum(indices_ranks[:] != indices_ranks_XY[:]), 'ranks.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ffbc46",
   "metadata": {},
   "source": [
    "We note a more significant change in the PageRanks of the existing pages resulting from the introduction of the new pages. It is crucial to consider the substantial number of nodes in the network. The PageRank of the newly added page aligns with the expected outcome from the PageRank calculation formula. Notably, the PageRank of X has visibly improved, while the PageRank of Y remains the same as that of page X in the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db7ccf",
   "metadata": {},
   "source": [
    "### Question 6. Maximizing PageRank for Page X with links from Pages Y and Z "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293a9dd",
   "metadata": {},
   "source": [
    "To maximize the PageRank of X, the links from both Y and Z are directed exclusively to X, aligning with the underlying logic of PageRank.\n",
    "     The implementation of this approach is encapsulated in the get_data_add_XYZ method, which adds nodes Y and Z, strategically configuring links to enhance the PageRank of X. The observed results confirm a significant improvement in the PageRank of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6e5ad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PageRank of page X is: 1.4374891233916276e-06\n",
      "The PageRank of page Y is: 5.32283146606264e-07\n",
      "The PageRank of page Z is: 5.32283146606264e-07\n",
      "Change in PageRank of the other pages due to the addition of the new pages: 6.654224222608326e-08\n",
      "Changes in: 78356 ranks.\n"
     ]
    }
   ],
   "source": [
    "# Similar analysis to previous questions for the addition of new pages X, Y, and Z and links\n",
    "\n",
    "G = construct_sparse_matrix('stanweb.dat')\n",
    "G_XYZ = construct_sparse_matrix_XYZ('stanweb.dat')\n",
    "\n",
    "ranks = PageRank_PowerMethod(G, 0.85, 1e-8)[0]\n",
    "ranks_XYZ = PageRank_PowerMethod(G_XYZ, 0.85, 1e-8)[0]\n",
    "\n",
    "pagerank_X = np.asarray(ranks_XYZ[G_XYZ.shape[0] - 1]).ravel()[0]\n",
    "print('The PageRank of page X is:', pagerank_X)\n",
    "\n",
    "pagerank_Y = np.asarray(ranks_XYZ[G_XYZ.shape[0] - 2]).ravel()[0]\n",
    "print('The PageRank of page Y is:', pagerank_Y)\n",
    "\n",
    "pagerank_Z = np.asarray(ranks_XYZ[G_XYZ.shape[0] - 3]).ravel()[0]\n",
    "print('The PageRank of page Z is:', pagerank_Z)\n",
    "\n",
    "ranks_XYZ = ranks_XYZ[:len(ranks)]\n",
    "ranks = np.asarray(ranks).ravel()\n",
    "ranks_XYZ = np.asarray(ranks_XYZ).ravel()\n",
    "\n",
    "difference = np.linalg.norm(ranks - ranks_XYZ)\n",
    "print('Change in PageRank of the other pages due to the addition of the new pages:', difference)\n",
    "\n",
    "indices_ranks = ranks.argsort()[-len(ranks):][::-1]\n",
    "indices_ranks_XYZ = ranks_XYZ.argsort()[-len(ranks_XYZ):][::-1]\n",
    "\n",
    "print('Changes in:', np.sum(indices_ranks[:] != indices_ranks_XYZ[:]), 'ranks.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895fc4f9",
   "metadata": {},
   "source": [
    "The PageRank of X has improved, while the PageRank of Y and Z remains the same as that of page Y in the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62637f8",
   "metadata": {},
   "source": [
    "### Question 7. PageRank Improvement Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e4a9a6",
   "metadata": {},
   "source": [
    "#### Links from X to Older, Popular Pages and Links from Y or Z to Older, Popular Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d814299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 most popular pages:\n",
      "[226410 234703 105606 241453 167294 198089  81434 214127  38341 245658\n",
      "  34572  89072  69357  67755 134831 231362 120707  62477 176789 259454\n",
      " 137631 247240 183003 221086  77998 181700  17780  96744 186749 225871\n",
      "  68888 136820 251795 272441  95162  84427 133796 119478 251989  93777\n",
      "  27903 272761 186901 205476   2259  98386  19188 213897 192928 151938\n",
      " 170451  67502 134374 117151 235495  60209 258347 151980  93988  84905\n",
      "  27114 132694 267491  50784  55787 137689 265386 204568 130831 270770\n",
      "  35083  58763  41116  65462  32103 271204 121420 248138  86239  63855\n",
      " 226289 165188  92640  20532 117863 281567 247251 243179 244194  19469\n",
      " 141369 116179 177013  14124  53054  82475   3163 153449  73529 262859]\n"
     ]
    }
   ],
   "source": [
    "# Determine the top 100 most popular pages based on in-degree\n",
    "incoming_edges = get_data('stanweb.dat')[2]\n",
    "list_degree = np.asarray([len(incoming_edges[x]) for x in sorted(list(incoming_edges.keys()))])\n",
    "\n",
    "indices_degree = list_degree.argsort()[-len(list_degree):][::-1]\n",
    "popular_100 = indices_degree[0:100]\n",
    "print(f'100 most popular pages:\\n{popular_100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed8a22eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we add links from X to older, popular pages the PageRank of page X is: 1.4374862565092927e-06\n",
      "If we add links from Y to older, popular pages the PageRank of page X is: 1.4374862565092927e-06\n"
     ]
    }
   ],
   "source": [
    "# Construct the adjacency matrix for the graph with links from X to the popular pages\n",
    "G_pop_1 = construct_sparse_matrix_popular_1('stanweb.dat', popular_100)\n",
    "\n",
    "# Construct the adjacency matrix for the graph with links from Y to the popular pages\n",
    "G_pop_2 = construct_sparse_matrix_popular_2('stanweb.dat', popular_100)\n",
    "\n",
    "# Compute PageRank for the graph with links from X to popular pages\n",
    "ranksp1 = PageRank_PowerMethod(G_pop_1, 0.85, 1e-8)[0]\n",
    "\n",
    "# Compute PageRank for the graph with links from Y to popular pages\n",
    "ranksp2 = PageRank_PowerMethod(G_pop_2, 0.85, 1e-8)[0]\n",
    "\n",
    "# Extract the PageRank of page X in the graph with links from X to popular pages\n",
    "pagerank_Xpop_1 = np.asarray(ranksp1[G_pop_1.shape[0] - 1]).ravel()[0]\n",
    "\n",
    "# Extract the PageRank of page X in the graph with links from Y to popular pages\n",
    "pagerank_Xpop_2 = np.asarray(ranksp2[G_pop_2.shape[0] - 1]).ravel()[0]\n",
    "\n",
    "print('If we add links from X to older, popular pages the PageRank of page X is:', pagerank_Xpop_1)\n",
    "print('If we add links from Y to older, popular pages the PageRank of page X is:', pagerank_Xpop_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058c9365",
   "metadata": {},
   "source": [
    "It appears that introducing additional links to either X or Y does not alter the results obtained from the power method. The rankings persist without significant changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd7f0a",
   "metadata": {},
   "source": [
    "### Question 8. New Node with Links to Top 10 Nodes and In-Links from 500 Nodes Below Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "787e2ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_G = PageRank_PowerMethod(G, 0.85, 1e-8)[0]\n",
    "ranks_G = np.asarray(ranks_G).ravel()\n",
    "nodes_G = ranks_G.argsort()[-n:][::-1]\n",
    "\n",
    "chosen_nodes = []\n",
    "\n",
    "# Select 500 nodes with ranks below the mean rank\n",
    "for x in range(0, 1000):\n",
    "    chosen_nodes.append(list(ranks_G).index(ranks_G[ranks_G < np.mean(ranks_G)][x]))\n",
    "\n",
    "chosen_nodes = list(set(chosen_nodes))[:500]\n",
    "\n",
    "# Create a dictionary to store the chosen and important nodes\n",
    "d={}\n",
    "\n",
    "d['chosen nodes'] = chosen_nodes\n",
    "\n",
    "d['important nodes'] = list(nodes_G[:10].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64df2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_8(data, d):\n",
    "    \"\"\"\n",
    "    Extracts and processes data from the given file.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the data file/ Name of the file (str).\n",
    "    - d: A dictionary containing information about the top 10 nodes and 500 nodes below average.\n",
    "\n",
    "    Returns:\n",
    "    - edges: The number of edges in the \"graph\".\n",
    "    - nodes: The number of nodes in the \"graph\".\n",
    "    - incoming_edges: Dictionary with webpages as keys and the count of their outgoing edges as values.\n",
    "    - outgoing_edges: Dictionary with webpages as keys and a list of incoming edges as values.\n",
    "    - source_pages: List of the origin pages.\n",
    "    - target_pages: List of the destination pages.\n",
    "    \"\"\"\n",
    "    source_pages, target_pages = [], []\n",
    "    incoming_edges, outgoing_edges = {}, {}\n",
    "    with open(data, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            source = int(line.split()[0]) - 1\n",
    "            destination = int(line.split()[1]) - 1\n",
    "\n",
    "            source_pages.append(source)\n",
    "            target_pages.append(destination)\n",
    "\n",
    "            if source not in outgoing_edges.keys():\n",
    "                outgoing_edges[source] = 1\n",
    "            else:\n",
    "                outgoing_edges[source] += 1\n",
    "\n",
    "            if destination not in incoming_edges.keys():\n",
    "                incoming_edges[destination] = [source]\n",
    "            else:\n",
    "                incoming_edges[destination].append(source)\n",
    "\n",
    "    for x in d['important nodes']:\n",
    "        source = 281903\n",
    "        destination = x\n",
    "        source_pages.append(source)\n",
    "        target_pages.append(destination)\n",
    "\n",
    "        if source not in outgoing_edges.keys():\n",
    "            outgoing_edges[source] = 1\n",
    "        else:\n",
    "            outgoing_edges[source] += 1\n",
    "\n",
    "        if destination not in incoming_edges.keys():\n",
    "            incoming_edges[destination] = [source]\n",
    "        else:\n",
    "            incoming_edges[destination].append(source)\n",
    "\n",
    "    for x in d['chosen nodes']:\n",
    "        source = x\n",
    "        destination = 281903 # New node is the last node #281903 \n",
    "        source_pages.append(source)\n",
    "        target_pages.append(destination)\n",
    "\n",
    "        if source not in outgoing_edges.keys():\n",
    "            outgoing_edges[source] = 1\n",
    "        else:\n",
    "            outgoing_edges[source] += 1\n",
    "\n",
    "        if destination not in incoming_edges.keys():\n",
    "            incoming_edges[destination] = [source]\n",
    "        else:\n",
    "            incoming_edges[destination].append(source)\n",
    "\n",
    "    edges = len(source_pages)\n",
    "    nodes = len(set(source_pages) | set(target_pages))\n",
    "\n",
    "    for node in range(nodes):\n",
    "        if node not in incoming_edges.keys():\n",
    "            incoming_edges[node] = []\n",
    "\n",
    "    return edges, nodes, incoming_edges, outgoing_edges, source_pages, target_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4af660ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sparse_matrix_8(data,d):\n",
    "    \"\"\"\n",
    "    Construct a sparse matrix from the given connectivity data.\n",
    "    (sparse matrix is used for memory efficiency)\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the input data file/ Data file name.\n",
    "    - d: A dictionary containing information about the top 10 nodes and 500 nodes below average.\n",
    "\n",
    "    Returns:\n",
    "    - sparse_matrix (sparse.csr_matrix): The constructed Sparse-CSR matrix - representation of the graph.\n",
    "    \"\"\"\n",
    "    edges, nodes, _, _, source_pages, target_pages = get_data_8(data,d)\n",
    "\n",
    "    edge_weights = [1]*edges\n",
    "    sparse_matrix = sparse.csr_matrix((edge_weights,\n",
    "                                      (target_pages, source_pages)),\n",
    "                                      shape=(nodes, nodes))\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6080558d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PageRank vector:\n",
      "\n",
      " [[5.32285116e-07]\n",
      " [1.17295980e-04]\n",
      " [8.25921488e-07]\n",
      " ...\n",
      " [1.80765246e-06]\n",
      " [1.47360002e-06]\n",
      " [1.05010089e-04]]\n",
      "\n",
      "Rank of the new node: 806\n"
     ]
    }
   ],
   "source": [
    "G_8 = construct_sparse_matrix_8('stanweb.dat',d)\n",
    "n = G_8.shape[0]\n",
    "\n",
    "ranks_G_8 = PageRank_PowerMethod(G_8, 0.85, 1e-8)[0]\n",
    "print(f'\\nPageRank vector:\\n\\n {ranks_G_8}')\n",
    "\n",
    "ranks_G_8 = np.asarray(ranks_G_8).ravel()\n",
    "nodes_G_8 = ranks_G_8.argsort()[-n:][::-1]\n",
    "\n",
    "print('\\nRank of the new node:',list(nodes_G_8).index(281903))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caae661",
   "metadata": {},
   "source": [
    "### Question 9. New Node with Links to Bottom 10 Nodes and In-Links from Top 10 Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fe4682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = construct_sparse_matrix('stanweb.dat')\n",
    "n = G.shape[0]\n",
    "\n",
    "ranks_G = PageRank_PowerMethod(G, 0.85, 1e-8)[0]\n",
    "ranks_G = np.asarray(ranks_G).ravel()\n",
    "nodes_G = ranks_G.argsort()[-n:][::-1]\n",
    "\n",
    "list(nodes_G[-10:].ravel())\n",
    "\n",
    "# Store the least and most important nodes from the original graph\n",
    "d1={}\n",
    "d1['least important nodes'] = list(nodes_G[-10:].ravel())\n",
    "d1['most important nodes'] = list(nodes_G[:10].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e072b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_9(data, d):\n",
    "    \"\"\"\n",
    "    Extracts and processes data from the given file.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the data file/ Name of the file (str).\n",
    "    - d: A dictionary containing information about the least and most important 10 nodes.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - edges: The number of edges in the \"graph\".\n",
    "    - nodes: The number of nodes in the \"graph\".\n",
    "    - incoming_edges: Dictionary with webpages as keys and the count of their outgoing edges as values.\n",
    "    - outgoing_edges: Dictionary with webpages as keys and a list of incoming edges as values.\n",
    "    - source_pages: List of the origin pages.\n",
    "    - target_pages: List of the destination pages.\n",
    "    \"\"\"\n",
    "    source_pages, target_pages = [], []\n",
    "    incoming_edges, outgoing_edges = {}, {}\n",
    "    with open(data, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            source = int(line.split()[0]) - 1\n",
    "            destination = int(line.split()[1]) - 1\n",
    "            source_pages.append(source)\n",
    "            target_pages.append(destination)\n",
    "\n",
    "            if source not in outgoing_edges.keys():\n",
    "                outgoing_edges[source] = 1\n",
    "            else:\n",
    "                outgoing_edges[source] += 1\n",
    "\n",
    "            if destination not in incoming_edges.keys():\n",
    "                incoming_edges[destination] = [source]\n",
    "            else:\n",
    "                incoming_edges[destination].append(source)\n",
    "\n",
    "    for x in d['most important nodes']:\n",
    "        source = x\n",
    "        destination = 281903\n",
    "        source_pages.append(source)\n",
    "        target_pages.append(destination)\n",
    "\n",
    "        if source not in outgoing_edges.keys():\n",
    "            outgoing_edges[source] = 1\n",
    "        else:\n",
    "            outgoing_edges[source] += 1\n",
    "\n",
    "        if destination not in incoming_edges.keys():\n",
    "            incoming_edges[destination] = [source]\n",
    "        else:\n",
    "            incoming_edges[destination].append(source)\n",
    "\n",
    "    for x in d['least important nodes']:\n",
    "        source = 281903\n",
    "        destination = x\n",
    "        source_pages.append(source)\n",
    "        target_pages.append(destination)\n",
    "\n",
    "        if source not in outgoing_edges.keys():\n",
    "            outgoing_edges[source] = 1\n",
    "        else:\n",
    "            outgoing_edges[source] += 1\n",
    "\n",
    "        if destination not in incoming_edges.keys():\n",
    "            incoming_edges[destination] = [source]\n",
    "        else:\n",
    "            incoming_edges[destination].append(source)\n",
    "\n",
    "    edges = len(source_pages)\n",
    "    nodes = len(set(source_pages) | set(target_pages))\n",
    "\n",
    "    for node in range(nodes):\n",
    "        if node not in incoming_edges.keys():\n",
    "            incoming_edges[node] = []\n",
    "\n",
    "    return edges, nodes, incoming_edges, outgoing_edges, source_pages, target_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bceccf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sparse_matrix_9(data,d):\n",
    "    \"\"\"\n",
    "    Construct a sparse matrix from the given connectivity data.\n",
    "    (sparse matrix is used for memory efficiency)\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the input data file/ Data file name.\n",
    "    - d: A dictionary containing information about the least and most important 10 nodes.\n",
    "    \n",
    "    Returns:\n",
    "    - sparse_matrix (sparse.csr_matrix): The constructed Sparse-CSR matrix - representation of the graph.\n",
    "    \"\"\"\n",
    "    edges, nodes, _, _, source_pages, target_pages = get_data_9(data,d)\n",
    "\n",
    "    edge_weights = [1]*edges\n",
    "    sparse_matrix = sparse.csr_matrix((edge_weights,\n",
    "                                      (target_pages, source_pages)),\n",
    "                                      shape=(nodes, nodes))\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25fd4b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank of the new node: 13\n"
     ]
    }
   ],
   "source": [
    "G_9 = construct_sparse_matrix_9('stanweb.dat',d1)\n",
    "n = G_9.shape[0]\n",
    "\n",
    "ranks_G_9 = PageRank_PowerMethod(G_9, 0.85, 1e-8)[0]\n",
    "\n",
    "ranks_G_9 = np.asarray(ranks_G_9).ravel()\n",
    "nodes_G_9= ranks_G_9.argsort()[-n:][::-1]\n",
    "\n",
    "print('\\nRank of the new node:',list(nodes_G_9).index(281903))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a7472ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " rank of most important nodes 1: 1\n",
      "\n",
      " rank of most important nodes 2: 0\n",
      "\n",
      " rank of most important nodes 3: 2\n",
      "\n",
      " rank of most important nodes 4: 5\n",
      "\n",
      " rank of most important nodes 5: 3\n",
      "\n",
      " rank of most important nodes 6: 4\n",
      "\n",
      " rank of most important nodes 7: 11\n",
      "\n",
      " rank of most important nodes 8: 12\n",
      "\n",
      " rank of most important nodes 9: 6\n",
      "\n",
      " rank of most important nodes 10: 9\n"
     ]
    }
   ],
   "source": [
    "print('\\n rank of most important nodes 1:',list(nodes_G_9).index(d1['most important nodes'][0]))\n",
    "print('\\n rank of most important nodes 2:',list(nodes_G_9).index(d1['most important nodes'][1]))\n",
    "print('\\n rank of most important nodes 3:',list(nodes_G_9).index(d1['most important nodes'][2]))\n",
    "print('\\n rank of most important nodes 4:',list(nodes_G_9).index(d1['most important nodes'][3]))\n",
    "print('\\n rank of most important nodes 5:',list(nodes_G_9).index(d1['most important nodes'][4]))\n",
    "print('\\n rank of most important nodes 6:',list(nodes_G_9).index(d1['most important nodes'][5]))\n",
    "print('\\n rank of most important nodes 7:',list(nodes_G_9).index(d1['most important nodes'][6]))\n",
    "print('\\n rank of most important nodes 8:',list(nodes_G_9).index(d1['most important nodes'][7]))\n",
    "print('\\n rank of most important nodes 9:',list(nodes_G_9).index(d1['most important nodes'][8]))\n",
    "print('\\n rank of most important nodes 10:',list(nodes_G_9).index(d1['most important nodes'][9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67b6eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " rank of least important node 1: 408\n",
      "\n",
      " rank of least important node 2: 409\n",
      "\n",
      " rank of least important node 3: 407\n",
      "\n",
      " rank of least important node 4: 403\n",
      "\n",
      " rank of least important node 5: 401\n",
      "\n",
      " rank of least important node 6: 402\n",
      "\n",
      " rank of least important node 7: 405\n",
      "\n",
      " rank of least important node 8: 406\n",
      "\n",
      " rank of least important node 9: 404\n",
      "\n",
      " rank of least important node 10: 410\n"
     ]
    }
   ],
   "source": [
    "print('\\n rank of least important node 1:',list(nodes_G_9).index(d1['least important nodes'][0]))\n",
    "print('\\n rank of least important node 2:',list(nodes_G_9).index(d1['least important nodes'][1]))\n",
    "print('\\n rank of least important node 3:',list(nodes_G_9).index(d1['least important nodes'][2]))\n",
    "print('\\n rank of least important node 4:',list(nodes_G_9).index(d1['least important nodes'][3]))\n",
    "print('\\n rank of least important node 5:',list(nodes_G_9).index(d1['least important nodes'][4]))\n",
    "print('\\n rank of least important node 6:',list(nodes_G_9).index(d1['least important nodes'][5]))\n",
    "print('\\n rank of least important node 7:',list(nodes_G_9).index(d1['least important nodes'][6]))\n",
    "print('\\n rank of least important node 8:',list(nodes_G_9).index(d1['least important nodes'][7]))\n",
    "print('\\n rank of least important node 9:',list(nodes_G_9).index(d1['least important nodes'][8]))\n",
    "print('\\n rank of least important node 10:',list(nodes_G_9).index(d1['least important nodes'][9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba37e62f",
   "metadata": {},
   "source": [
    "### PageRank Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d5051",
   "metadata": {},
   "source": [
    "In this problem, we delve into a challenging scenario faced by a company aiming to boost its website's prominence in Google searches by reaching the top 1% of the Pagerank ranking. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94590f",
   "metadata": {},
   "source": [
    "The primary challenge revolves around two key strategies:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25dff3a",
   "metadata": {},
   "source": [
    "**1. Linking to High Pagerank Sites.**\n",
    "Cost: (285000 - i + 1)^2 cents for each link added to a page with a score of i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3dbf61",
   "metadata": {},
   "source": [
    "**2. Creating New Satellite Sites.**\n",
    "Cost: 100 euros per page for establishing new satellite sites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739c3b9",
   "metadata": {},
   "source": [
    "Our objective is to analyze and propose a solution that strikes a balance between effectiveness and cost-efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb4e07",
   "metadata": {},
   "source": [
    "### First Strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6125ecf9",
   "metadata": {},
   "source": [
    "We randomly selected 1000 pages with a rank near the mean and linked our website to one of these selected pages. If we didn't achieve the desired result, we continued to increase the number of linked pages until we reached a rank of 1%. This process required us to link to a total of 36 nodes for 2,892,276,729,591 cents. However, it's important to note that the choice of higher-ranked pages was random and this method can be quite costly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97bff4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph and compute PageRank\n",
    "G = construct_sparse_matrix('stanweb.dat')\n",
    "n = G.shape[0]\n",
    "ranks = PageRank_PowerMethod(G, 0.85, 1e-8)[0]\n",
    "ranks = np.asarray(ranks).ravel()\n",
    "nodes_alpha_85 = ranks.argsort()[-n:][::-1]\n",
    "\n",
    "# Select candidate nodes based on proximity to the mean PageRank value\n",
    "chosen_nodes = []\n",
    "for x in range(0, 1000):\n",
    "    chosen_nodes.append(list(ranks).index(ranks[list(ranks<(np.mean(ranks)+np.mean(ranks)/10)) and list(ranks>(np.mean(ranks)-np.mean(ranks)/10))][x]))\n",
    "    \n",
    "chosen_nodes=list(set(chosen_nodes))[:36]\n",
    "\n",
    "# Create a dictionary to store \"candidate\" nodes\n",
    "d2={}\n",
    "d2['candidate'] = chosen_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de284c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_strategy_1(data, d):\n",
    "    \"\"\"\n",
    "    Extracts and processes data from the given file, incorporating link additions for the first strategy.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the data file/ Name of the file (str).\n",
    "    - d: A dictionary containing information about the nodes selected for link additions.\n",
    "\n",
    "    Returns:\n",
    "    - edges: The number of edges in the \"graph\".\n",
    "    - nodes: The number of nodes in the \"graph\".\n",
    "    - incoming_edges: Dictionary with webpages as keys and the count of their outgoing edges as values.\n",
    "    - outgoing_edges: Dictionary with webpages as keys and a list of incoming edges as values.\n",
    "    - source_pages: List of the origin pages.\n",
    "    - target_pages: List of the destination pages.\n",
    "    \"\"\"\n",
    "    source_pages, target_pages = [], []\n",
    "    incoming_edges, outgoing_edges = {}, {}\n",
    "\n",
    "    with open(data, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            source = int(line.split()[0]) - 1\n",
    "            destination = int(line.split()[1]) - 1\n",
    "\n",
    "            source_pages.append(source)\n",
    "            target_pages.append(destination)\n",
    "\n",
    "            if source not in outgoing_edges.keys():\n",
    "                outgoing_edges[source] = 1\n",
    "            else:\n",
    "                outgoing_edges[source] += 1\n",
    "\n",
    "            if destination not in incoming_edges.keys():\n",
    "                incoming_edges[destination] = [source]\n",
    "            else:\n",
    "                incoming_edges[destination].append(source)\n",
    "\n",
    "    for x in d['candidate']:\n",
    "        source = x\n",
    "        destination = 281903 # Our Website (Last Node - Website)\n",
    "        source_pages.append(source)\n",
    "        target_pages.append(destination)\n",
    "\n",
    "        if source not in outgoing_edges.keys():\n",
    "            outgoing_edges[source] = 1\n",
    "        else:\n",
    "            outgoing_edges[source] += 1\n",
    "\n",
    "        if destination not in incoming_edges.keys():\n",
    "            incoming_edges[destination] = [source]\n",
    "        else:\n",
    "            incoming_edges[destination].append(source)\n",
    "\n",
    "\n",
    "    edges = len(source_pages)\n",
    "    nodes = len(set(source_pages) | set(target_pages))\n",
    "\n",
    "    for node in range(nodes):\n",
    "        if node not in incoming_edges.keys():\n",
    "            incoming_edges[node] = []\n",
    "\n",
    "    # We only need the edges and nodes of the function to implement the strategy\n",
    "    # We compute outgoing and incoming edges dictionaries for potential further analysis\n",
    "    return edges, nodes, incoming_edges, outgoing_edges, source_pages, target_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac342509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sparse_matrix_strategy_1(data,d):\n",
    "    \"\"\"\n",
    "    Construct a sparse matrix from the given connectivity data.\n",
    "    (sparse matrix is used for memory efficiency)\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the input data file/ Data file name.\n",
    "    - d: A dictionary containing information about the nodes selected for link additions.\n",
    "\n",
    "    Returns:\n",
    "    - sparse_matrix (sparse.csr_matrix): The constructed Sparse-CSR matrix - representation of the graph.\n",
    "    \"\"\"\n",
    "    edges, nodes, _, _, source_pages, target_pages = get_data_strategy_1(data,d)\n",
    "\n",
    "    edge_weights = [1]*edges\n",
    "    sparse_matrix = sparse.csr_matrix((edge_weights,\n",
    "                                      (target_pages, source_pages)),\n",
    "                                      shape=(nodes, nodes))\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6378f826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank of the Website: 2802\n",
      "\n",
      "Our Website is at the Top 0.99 % in PageRank Ranking.\n",
      "\n",
      "Total cost for the first strategy: 2892276729591 cents\n"
     ]
    }
   ],
   "source": [
    "G_S1 = construct_sparse_matrix_strategy_1('stanweb.dat',d2)\n",
    "n = G_S1.shape[0]\n",
    "\n",
    "ranks_S1 = PageRank_PowerMethod(G_S1, 0.85, 1e-8)[0]\n",
    "ranks_S1 = np.asarray(ranks_S1).ravel()\n",
    "nodes_S1 = ranks_S1.argsort()[-n:][::-1]\n",
    "\n",
    "print('\\nRank of the Website:',list(nodes_S1).index(281903))\n",
    "percentage_S1 = (list(nodes_S1).index(281903)/281903)*100\n",
    "percentage_S1 = format(percentage_S1,'.2f')\n",
    "print('\\nOur Website is at the Top', percentage_S1,'% in PageRank Ranking.')\n",
    "cost = 0\n",
    "for x in chosen_nodes:\n",
    "    cost += (285000 - x + 1)**2   \n",
    "print('\\nTotal cost for the first strategy:', cost,'cents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e5dd4",
   "metadata": {},
   "source": [
    "### Second Strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3d2b9",
   "metadata": {},
   "source": [
    "We created 75 new sites without using the first variation (If we didn't initially achieve the desired result, we continued to create additional sites until reaching the top 1%). We reached the objective for only 7500 euros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b0ddb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store new nodes\n",
    "d={}\n",
    "number_new_nodes = 75\n",
    "d['new node'] = []\n",
    "for i in range(number_new_nodes):\n",
    "    d['new node'].append(281903 + i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "011a4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_strategy_2(data,d):\n",
    "    \"\"\"\n",
    "    Extracts and processes data from the given file, incorporating new nodes for the second strategy.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the data file/ Name of the file (str).\n",
    "    - d: A dictionary containing information about the new nodes.\n",
    "\n",
    "    Returns:\n",
    "    - edges: The number of edges in the \"graph\".\n",
    "    - nodes: The number of nodes in the \"graph\".\n",
    "    - incoming_edges: Dictionary with webpages as keys and the count of their outgoing edges as values.\n",
    "    - outgoing_edges: Dictionary with webpages as keys and a list of incoming edges as values.\n",
    "    - source_pages: List of the origin pages.\n",
    "    - target_pages: List of the destination pages.\n",
    "    \"\"\"\n",
    "    source_pages, target_pages = [], []\n",
    "    incoming_edges, outgoing_edges = {}, {}\n",
    "\n",
    "    with open(data, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            source = int(line.split()[0]) - 1\n",
    "            destination = int(line.split()[1]) - 1\n",
    "\n",
    "            source_pages.append(source)\n",
    "            target_pages.append(destination)\n",
    "\n",
    "            if source not in outgoing_edges.keys():\n",
    "                outgoing_edges[source] = 1\n",
    "            else:\n",
    "                outgoing_edges[source] += 1\n",
    "\n",
    "            if destination not in incoming_edges.keys():\n",
    "                incoming_edges[destination] = [source]\n",
    "            else:\n",
    "                incoming_edges[destination].append(source)\n",
    "\n",
    "    for x in d['new node']:\n",
    "        source = x\n",
    "        destination = 281903 # Our Website\n",
    "        source_pages.append(source)\n",
    "        target_pages.append(destination)\n",
    "\n",
    "        if source not in outgoing_edges.keys():\n",
    "            outgoing_edges[source] = 1\n",
    "        else:\n",
    "            outgoing_edges[source] += 1\n",
    "\n",
    "        if destination not in incoming_edges.keys():\n",
    "            incoming_edges[destination] = [source]\n",
    "        else:\n",
    "            incoming_edges[destination].append(source)\n",
    "\n",
    "    edges = len(source_pages)\n",
    "    nodes = len(set(source_pages) | set(target_pages))\n",
    "\n",
    "    for node in range(nodes):\n",
    "        if node not in incoming_edges.keys():\n",
    "            incoming_edges[node] = []\n",
    "\n",
    "    return edges, nodes, incoming_edges, outgoing_edges, source_pages, target_pages\n",
    "\n",
    "def construct_sparse_matrix_strategy_2(data,d):\n",
    "    \"\"\"\n",
    "    Construct a sparse matrix from the given connectivity data.\n",
    "    (sparse matrix is used for memory efficiency)\n",
    "\n",
    "    Parameters:\n",
    "    - data: The path to the input data file/ Data file name.\n",
    "    - d: A dictionary containing information about the new nodes.\n",
    "\n",
    "    Returns:\n",
    "    - sparse_matrix (sparse.csr_matrix): The constructed Sparse-CSR matrix - representation of the graph.\n",
    "    \"\"\"\n",
    "    edges, nodes, _, _, source_pages, target_pages = get_data_strategy_2(data,d)\n",
    "\n",
    "    edge_weights = [1]*edges\n",
    "    sparse_matrix = sparse.csr_matrix((edge_weights,\n",
    "                                      (target_pages, source_pages)),\n",
    "                                      shape=(nodes, nodes))\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ade8d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rank of the Website: 2803\n",
      "\n",
      "Our Website is at the Top 0.99 % in PageRank Ranking.\n",
      "\n",
      "Total cost for the second strategy: 7500 euros\n"
     ]
    }
   ],
   "source": [
    "G_S2 = construct_sparse_matrix_strategy_2('stanweb.dat',d)\n",
    "n = G_S2.shape[0]\n",
    "ranks_S2 = PageRank_PowerMethod(G_S2, 0.85, 1e-8)[0]\n",
    "ranks_S2 = np.asarray(ranks_S2).ravel()\n",
    "nodes_S2 = ranks_S2.argsort()[-n:][::-1]\n",
    "\n",
    "print('\\nRank of the Website:',list(nodes_S2).index(281903))\n",
    "percentage_S2 = (list(nodes_S2).index(281903)/281903)*100\n",
    "percentage_S2 = format(percentage_S2,'.2f')\n",
    "print('\\nOur Website is at the Top', percentage_S2,'% in PageRank Ranking.')\n",
    "print('\\nTotal cost for the second strategy:', number_new_nodes * 100,'euros')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b8c5b",
   "metadata": {},
   "source": [
    "### Conclusion and Recommended Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5276a1c",
   "metadata": {},
   "source": [
    "After evaluating the cost-effectiveness of two strategies to boost the company's website to the top 1% of the Pagerank ranking, we found the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd26a7b",
   "metadata": {},
   "source": [
    "**Linking to High Pagerank Sites:**\n",
    "   - Total Cost: 2,892,276,729,591 cents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c10e70",
   "metadata": {},
   "source": [
    "**Creating New Affiliated Nodes:**\n",
    "   - Total Cost: 7,500 euros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caee0ed",
   "metadata": {},
   "source": [
    "The analysis indicates that the second approach, creating new affiliated nodes, is more cost-effective. This approach aims to achieve the desired ranking while minimizing costs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166e9e80",
   "metadata": {},
   "source": [
    "Further adjustments can be made based on budget constraints and specific goals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65a5f49",
   "metadata": {},
   "source": [
    "(Also, a hybrid approach that combines linking to high Pagerank sites and creating new nodes/sites could also be used)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
